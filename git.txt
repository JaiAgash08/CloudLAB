import ollama

# Example past conversations
past_examples = [
    {"customer": "I can't log in to my account.", "agent": "I'm sorry to hear that. Please try resetting your password using the 'Forgot Password' link on the login page."},
    {"customer": "When will my order arrive?", "agent": "Your order is expected to arrive within 3-5 business days. You can track it using the tracking number provided in your confirmation email."},
    {"customer": "I received the wrong item.", "agent": "I apologize for the inconvenience. Please contact our support team with your order number, and we'll arrange for a replacement or refund."}
]

def build_prompt(new_query: str) -> str:
    examples_text = "\n\n".join(
        f"Customer: {ex['customer']}\nAgent: {ex['agent']}"
        for ex in past_examples
    )

    prompt = f"""
You are a polite and professional customer support assistant.
Here are some past conversations:
{examples_text}
Now, draft a helpful reply to the NEW customer message:
Customer: {new_query}
Reply:
"""
    return prompt.strip()


def suggest_reply(new_query: str) -> str:
    prompt = build_prompt(new_query)
    response = ollama.chat(
        model="qwen3-coder:480b-cloud",
        messages=[
            {"role": "system", "content": "You are a helpful customer support assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response["message"]["content"].strip()


if __name__ == "__main__":
    while True:
        new_query = input("Enter customer message (or 'exit' to quit): ")
        if new_query.lower() == "exit":
            print("Goodbye!")
            break
        reply = suggest_reply(new_query)
        print("Suggested Reply:")
        print(reply)







import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import gradio as gr
import ollama
import json


data = {
    "title": [
        "Interstellar",
        "Inception"
    ],
    "description": [
        "A team of explorers travel through a wormhole in space to ensure humanity's survival.",
        "A thief who enters people's dreams to steal secrets."
    ],
    "genres": [
        "Sci-Fi, Adventure, Drama",
        "Sci-Fi, Action, Thriller"
    ]
}


df = pd.DataFrame(data)


df["combined"] = df["description"] + " " + df["genres"]

# Create TF-IDF vectors
vectorizer = TfidfVectorizer(stop_words="english")
item_vectors = vectorizer.fit_transform(df["combined"])



def parse_preferences_with_ollama(user_text):
    prompt = f"""
    Extract weighted keywords (0 to 1) from this user preference:
    "{user_text}"
    Return JSON like {{"sci-fi": 0.9, "drama": 0.8}}
    """
    response = ollama.chat(model="qwen3-coder:480b-cloud", messages=[{"role": "user", "content": prompt}])
    content = response['message']['content']
    # Remove markdown code blocks if present
    if content.startswith("```json"):
        content = content[7:]
    if content.endswith("```"):
        content = content[:-3]
    content = content.strip()
    try:
        weights = json.loads(content)
    except Exception:
        weights = {}
    return weights



def recommend_items(user_text, top_n=3):
    preferences = parse_preferences_with_ollama(user_text)
    if not preferences:
        user_keywords = user_text
    else:
        user_keywords = " ".join(
            [f"{word} " * int(weight * 10) for word, weight in preferences.items()]
        )

    user_vec = vectorizer.transform([user_keywords])
    similarities = cosine_similarity(user_vec, item_vectors).flatten()
    top_indices = similarities.argsort()[-top_n:][::-1]
    recommendations = df.iloc[top_indices][["title", "genres", "description"]]
    return recommendations



def gradio_recommender(user_input):
    recs = recommend_items(user_input)
    output = ""
    for _, row in recs.iterrows():
        output += f"**{row['title']}**\n*{row['genres']}*\n{row['description']}\n\n"
    return output



interface = gr.Interface(
    fn=gradio_recommender,
    inputs=gr.Textbox(
        label="Describe what you like",
        placeholder="e.g., I enjoy thoughtful sci-fi stories with deep characters"
    ),
    outputs=gr.Markdown(label="Recommended Movies"),
    title="Personalized Recommendation System",
    description="Enter your movie preferences in natural language and get top personalized recommendations."
)


interface.launch()







import gradio as gr
import requests
from PyPDF2 import PdfReader
from docx import Document
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
import json
import os

# Ollama Configuration
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "qwen3-coder:480b-cloud"


# -------------------- TEXT EXTRACTION --------------------
def extract_text(file):
    filename = file.name
    text = ""

    if filename.endswith(".pdf"):
        reader = PdfReader(filename)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"

    elif filename.endswith(".docx"):
        doc = Document(filename)
        text = "\n".join([p.text for p in doc.paragraphs if p.text.strip() != ""])

    elif filename.endswith(".txt"):
        with open(filename, "r", encoding="utf-8") as f:
            text = f.read()
    else:
        return "Unsupported file type."

    return text.strip()


# -------------------- OLLAMA GENERATION --------------------
def ollama_generate(prompt, max_tokens=1000):
    try:
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "temperature": 0.2,
                "max_tokens": max_tokens,
                "stream": True,
            },
            stream=True,
        )

        full_output = ""
        for line in response.iter_lines(decode_unicode=True):
            if not line:
                continue
            try:
                obj = json.loads(line)
                if "response" in obj:
                    full_output += obj["response"]
                if obj.get("done", False):
                    break
            except json.JSONDecodeError:
                continue

        return full_output.strip() if full_output else "No response content."

    except Exception as e:
        return f"Error connecting to Ollama API: {e}"


# -------------------- DOCUMENT ANALYSIS --------------------
def analyze_document(file, summary_level, user_question):
    text = extract_text(file)
    if text.startswith("Unsupported"):
        return text, "", ""

    if summary_level == "Short":
        level_instruction = "Summarize this in 1‚Äì2 sentences capturing only the main idea."
    elif summary_level == "Medium":
        level_instruction = "Summarize this in one paragraph highlighting all key points."
    else:
        level_instruction = "Provide a detailed section-wise summary with headings and key points."

    summary_prompt = f"""
You are an AI legal & literary content assistant.
Analyze and summarize the document at the {summary_level} level.

{level_instruction}

Document:
{text}
"""
    summary = ollama_generate(summary_prompt, max_tokens=800)

    # Optional Q&A
    answer = ""
    if user_question and user_question.strip():
        qa_prompt = f"""
You are an AI assistant analyzing the document below.
Answer this specific question clearly and concisely.

Question: {user_question}

Document:
{text}
"""
        answer = ollama_generate(qa_prompt, max_tokens=400)

    # Extract keywords
    key_prompt = f"""
Extract the main keywords, legal clauses, or important entities from this text.
Provide them as a comma-separated list.

Document:
{text}
"""
    keywords = ollama_generate(key_prompt, max_tokens=500)

    return summary, answer, keywords


# -------------------- EXPORT FUNCTION --------------------
def export_content(summary, answer, keywords, format_type):
    output_path = f"output.{format_type}"

    if format_type == "pdf":
        c = canvas.Canvas(output_path, pagesize=letter)
        y = 750
        c.setFont("Helvetica-Bold", 14)
        c.drawString(100, y, "AI Document Analysis")
        y -= 30

        c.setFont("Helvetica", 12)
        c.drawString(100, y, "Summary:")
        y -= 20
        for line in summary.split("\n"):
            c.drawString(100, y, line)
            y -= 15

        if answer:
            y -= 20
            c.drawString(100, y, "Q&A:")
            y -= 20
            for line in answer.split("\n"):
                c.drawString(100, y, line)
                y -= 15

        y -= 20
        c.drawString(100, y, "Keywords:")
        y -= 20
        for line in keywords.split(","):
            c.drawString(100, y, line.strip())
            y -= 15

        c.save()

    elif format_type == "docx":
        doc = Document()
        doc.add_heading("AI Document Analysis", level=1)
        doc.add_heading("Summary", level=2)
        doc.add_paragraph(summary)

        if answer:
            doc.add_heading("Q&A", level=2)
            doc.add_paragraph(answer)

        doc.add_heading("Keywords / Key Clauses", level=2)
        doc.add_paragraph(keywords)
        doc.save(output_path)

    elif format_type == "txt":
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"SUMMARY:\n{summary}\n\nQ&A:\n{answer}\n\nKEYWORDS:\n{keywords}")

    return f"Exported successfully as {output_path}"


# -------------------- GRADIO INTERFACE --------------------
def main_ui():
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("# üß† AI Document Analyzer: Summarize, Query, Export")
        gr.Markdown(
            "Upload **PDF**, **DOCX**, or **TXT** files to analyze. "
            "You can select summary level, ask questions, and export results."
        )

        with gr.Tab("üìÑ Upload & Analyze"):
            file_input = gr.File(file_types=[".pdf", ".docx", ".txt"], label="Upload Document")
            summary_level = gr.Radio(
                ["Short", "Medium", "Detailed"],
                label="Select Summary Level",
                value="Medium"
            )
            user_question = gr.Textbox(
                lines=2,
                placeholder="Optional: Ask a question about the document"
            )
            analyze_button = gr.Button("Analyze Document")

        with gr.Tab("üìù Summary"):
            summary_output = gr.Textbox(label="Generated Summary", lines=10)

        with gr.Tab("üí¨ Q&A"):
            qa_output = gr.Textbox(label="Answer", lines=6)

        with gr.Tab("üîë Keywords / Clauses"):
            keyword_output = gr.Textbox(label="Extracted Keywords or Clauses", lines=8)

        with gr.Tab("üì§ Export & Reporting"):
            export_format = gr.Radio(
                ["pdf", "docx", "txt"],
                label="Select Export Format",
                value="pdf"
            )
            export_button = gr.Button("Export Result")
            export_message = gr.Textbox(label="Export Status")

        analyze_button.click(
            fn=analyze_document,
            inputs=[file_input, summary_level, user_question],
            outputs=[summary_output, qa_output, keyword_output],
        )

        export_button.click(
            fn=export_content,
            inputs=[summary_output, qa_output, keyword_output, export_format],
            outputs=export_message,
        )

    demo.launch(share=False)


if __name__ == "__main__":
    main_ui()





import requests
import datetime

# Ollama API configuration
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "qwen3-coder:480b-cloud"

# Basic knowledge base for quick responses
knowledge_base = {
    "cold": {
        "keywords": ["sneeze", "runny nose", "congestion", "cough"],
        "advice": "It may be a common cold. Rest, drink warm fluids, and consider over-the-counter cold remedies.",
        "severity": "Mild"
    },
    "fever": {
        "keywords": ["high temperature", "fever", "chills", "body ache"],
        "advice": "It could be a mild viral fever. Stay hydrated and rest. If it persists, consult a doctor.",
        "severity": "Moderate"
    },
}

# Store conversation history
conversation_history = []


def check_knowledge_base(symptoms):
    """Checks if symptoms match any known conditions in the knowledge base."""
    for condition, data in knowledge_base.items():
        if any(word in symptoms.lower() for word in data["keywords"]):
            return {
                "condition": condition,
                "advice": data["advice"],
                "severity": data["severity"]
            }
    return None


def ollama_response(symptoms):
    """Uses Ollama model to generate a medical analysis response."""
    prompt = f"""
You are an AI medical assistant.
Analyze the following symptoms and provide:
1. Possible common causes.
2. Basic home care advice.
3. When to seek professional help.
4. Always include this disclaimer:
   "I am not a doctor. Consult a professional for accurate diagnosis."

Symptoms: {symptoms}
"""
    payload = {"model": MODEL_NAME, "prompt": prompt, "stream": False}

    try:
        response = requests.post(OLLAMA_URL, json=payload)
        if response.status_code == 200:
            return response.json().get("response", "").strip()
        else:
            return f"Error from Ollama: {response.text}"
    except Exception as e:
        return f"Error connecting to Ollama: {e}"


def medical_assistant(symptoms):
    """Main function to handle symptom input and response generation."""
    record = {"user": symptoms}

    kb_result = check_knowledge_base(symptoms)
    if kb_result:
        response = (
            f"Possible Condition: {kb_result['condition'].title()}\n"
            f"Severity: {kb_result['severity']}\n"
            f"Advice: {kb_result['advice']}"
        )

        if kb_result["severity"].lower() == "severe":
            response += "\n‚ö†Ô∏è EMERGENCY ALERT: Seek immediate medical attention!"
    else:
        response = ollama_response(symptoms)

    record["assistant"] = response
    conversation_history.append(record)
    return response


def export_health_report():
    """Exports the entire chat history as a text report."""
    if not conversation_history:
        print("No conversation history to export.")
        return

    filename = f"health_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write("AI-Powered Medical Assistant ‚Äì Health Report\n")
        f.write("=" * 60 + "\n\n")

        for i, item in enumerate(conversation_history, 1):
            f.write(f"Interaction {i}\n")
            f.write(f"User: {item['user']}\n")
            f.write(f"Assistant: {item['assistant']}\n")
            f.write("-" * 60 + "\n")

    print(f"‚úÖ Health report exported as {filename}")


# Main program
if __name__ == "__main__":
    print("ü©∫ AI-Powered Medical Assistant")
    print("Type 'exit' to quit, or 'report' to export your health summary.\n")

    while True:
        user_input = input("Enter your symptoms: ").strip()

        if user_input.lower() == "exit":
            print("Goodbye! Stay healthy. üëã")
            break
        elif user_input.lower() == "report":
            export_health_report()
            continue

        print("\nAssistant Response:\n")
        print(medical_assistant(user_input))
        print("\n" + "=" * 60 + "\n")
